{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torcheval\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataset import PlantData, PlantDataTest\n",
    "from scipy.stats import zscore\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.join('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(dir_path, 'train.csv'))\n",
    "test_data = pd.read_csv(os.path.join(dir_path, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, train_size=0.8, shuffle=False, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25120/3132992897.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  means = torch.tensor(train_data.mean(axis=0), dtype=torch.float32)\n",
      "/tmp/ipykernel_25120/3132992897.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  stds = torch.tensor(train_data.std(axis=0), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "means = torch.tensor(train_data.mean(axis=0), dtype=torch.float32)\n",
    "stds = torch.tensor(train_data.std(axis=0), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transforms\n",
    "\n",
    "data_transform = torchvision.transforms.Lambda(\n",
    "    lambda x : (x - means[1:-6])/(stds[1:-6])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target transforms\n",
    "\n",
    "target_transforms = torchvision.transforms.Lambda(\n",
    "    lambda x : (x - means[-6:])/(stds[-6:])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image transforms\n",
    "image_train_data_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ConvertImageDtype(dtype=torch.float32),\n",
    "    torchvision.transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.0), ratio=(1, 1)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomVerticalFlip(),\n",
    "    torchvision.transforms.RandomRotation(15),\n",
    "    torchvision.transforms.ColorJitter(0.2, 0.2, 0.2),\n",
    "    # torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Lambda(\n",
    "        lambda x: torch.clamp(x,min=0, max=1)\n",
    "    ),\n",
    "    # use resnet normalization\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    torchvision.transforms.RandomErasing(p=0.2, scale=(0.01, 0.02), ratio=(0.1, 4), value=0)\n",
    "])\n",
    "\n",
    "image_test_data_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ConvertImageDtype(dtype=torch.float32),\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    # torchvision.transforms.ToTensor(),\n",
    "    # use resnet normalization\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "\n",
    "train_dataset = PlantData(train_data,\n",
    "                          os.path.join(dir_path, 'train_images'),\n",
    "                          image_train_data_transforms,\n",
    "                          data_transform,\n",
    "                          target_transforms)\n",
    "val_dataset = PlantData(val_data,\n",
    "                          os.path.join(dir_path,  'train_images'),\n",
    "                          image_test_data_transforms,\n",
    "                          data_transform,\n",
    "                          target_transforms)\n",
    "test_dataset = PlantDataTest(test_data,\n",
    "                             os.path.join(dir_path,  'test_images'),\n",
    "                             image_test_data_transforms,\n",
    "                             data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               drop_last=True,\n",
    "                                               num_workers=1,\n",
    "                                               pin_memory=False)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             shuffle=False,\n",
    "                                             drop_last=False,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=False)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=1,\n",
    "                                              shuffle=False,\n",
    "                                              drop_last=False,\n",
    "                                              num_workers=1,\n",
    "                                              pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25120/2516471493.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model1 = torch.load('results_during_training/swin_v2_b_batch_seed1_model_epoch19.pth')\n"
     ]
    }
   ],
   "source": [
    "model1 = torch.load('results_during_training/swin_v2_b_batch_seed1_model_epoch20.pth')\n",
    "model1.cuda()\n",
    "model1.base_model = model1.base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransferSWINBatch(\n",
       "  (base_model): SwinTransformer(\n",
       "    (features): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): Permute()\n",
       "        (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=4, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=4, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.021739130434782608, mode=row)\n",
       "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): PatchMergingV2(\n",
       "        (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06521739130434782, mode=row)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): PatchMergingV2(\n",
       "        (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10869565217391304, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.15217391304347827, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1956521739130435, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.21739130434782608, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.2391304347826087, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.2608695652173913, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.2826086956521739, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.30434782608695654, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.32608695652173914, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.34782608695652173, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.3695652173913043, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.391304347826087, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.41304347826086957, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.43478260869565216, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.45652173913043476, mode=row)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): PatchMergingV2(\n",
       "        (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=32, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.4782608695652174, mode=row)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=32, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.5, mode=row)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (permute): Permute()\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (fc_extra1): Linear(in_features=163, out_features=1024, bias=True)\n",
       "  (fc_extra2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (batchnorm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc_combined1): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "  (fc_combined2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc_combined3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc_combined4): Linear(in_features=256, out_features=6, bias=True)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (dropout3): Dropout(p=0.2, inplace=False)\n",
       "  (dropout4): Dropout(p=0.2, inplace=False)\n",
       "  (batchnorm2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25120/3340248915.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2 = torch.load('results_during_training/dino_big_seed0_model_epoch10.pth')\n",
      "/home/eganjido/alicia/project/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model2 = torch.load('results_during_training/dino_big_seed0_model_epoch11.pth')\n",
    "model2.cuda()\n",
    "model2.base_model = model2.base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torcheval.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics1 = []\n",
    "metrics2 = []\n",
    "metricsavg = []\n",
    "for i in range(1,7):\n",
    "    metrics1.append(torcheval.metrics.R2Score())\n",
    "    metrics2.append(torcheval.metrics.R2Score())\n",
    "    metricsavg.append(torcheval.metrics.R2Score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eganjido/alicia/project/dataset.py:25: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  additional = self.data_transform(torch.tensor(self.csv.iloc[index, 1:-6], dtype=torch.float32))\n",
      "/home/eganjido/alicia/project/dataset.py:27: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  targets = self.targets_transform(torch.tensor(self.csv.iloc[index, -6:], dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (images, extra, targets) in enumerate(val_dataloader):\n",
    "    images = images.to(device)\n",
    "    extra = extra.to(device)\n",
    "    # targets = targets.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output1 = model1.forward(images,extra).detach().cpu()\n",
    "        output2 = model2.forward(images,extra).detach().cpu()\n",
    "\n",
    "    for i in range(1,7):\n",
    "        metrics1[i-1].update(output1[:,-i], targets[:,-i])\n",
    "        metrics2[i-1].update(output2[:,-i], targets[:,-i])\n",
    "        metricsavg[i-1].update((output2[:,-i]+output1[:,-i])/2, targets[:,-i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1 -- ViT DINO: 0.322124719619751, SwinV2 0.3162527084350586 Avg 0.3295576572418213\n",
      "Feature 2 -- ViT DINO: 0.19461798667907715, SwinV2 0.193059504032135 Avg 0.2053646445274353\n",
      "Feature 3 -- ViT DINO: 0.22174817323684692, SwinV2 0.20802748203277588 Avg 0.23242777585983276\n",
      "Feature 4 -- ViT DINO: 0.4617008566856384, SwinV2 0.4793391227722168 Avg 0.48964691162109375\n",
      "Feature 5 -- ViT DINO: 0.2957690954208374, SwinV2 0.29596614837646484 Avg 0.30895566940307617\n",
      "Feature 6 -- ViT DINO: 0.27881789207458496, SwinV2 0.2786998152732849 Avg 0.29363858699798584\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(f'Feature {i+1} -- ViT DINO: {metrics1[i].compute()}, SwinV2 {metrics2[i].compute()} Avg {metricsavg[i].compute()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.322124719619751, 0.19461798667907715, 0.22174817323684692, 0.4617008566856384, 0.2957690954208374, 0.27881789207458496]\n",
      "[0.3162527084350586, 0.193059504032135, 0.20802748203277588, 0.4793391227722168, 0.29596614837646484, 0.2786998152732849]\n",
      "[0.3295576572418213, 0.2053646445274353, 0.23242777585983276, 0.48964691162109375, 0.30895566940307617, 0.29363858699798584]\n"
     ]
    }
   ],
   "source": [
    "print([m.compute().detach().item() for m in metrics1])\n",
    "print([m.compute().detach().item() for m in metrics2])\n",
    "print([m.compute().detach().item() for m in metricsavg])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
